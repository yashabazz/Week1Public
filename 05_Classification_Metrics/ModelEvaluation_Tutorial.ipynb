{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModelEvaluation_Tutorial",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "57SCOcInULbj"
      },
      "source": [
        "# Model Evaluation Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3sZdLALtSf_i"
      },
      "source": [
        "## Topics\n",
        "*  Motivation for model evaluation\n",
        "*  Model training procedure\n",
        "*  Model evaluation metrics:\n",
        " *  Accuracy\n",
        " *  Confusion matrix\n",
        " *  Precision, Recall, F1\n",
        " *  Receiver Operating Characteristic (ROC) Curves\n",
        " *  Area Under the Curve (AUC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oBz9FrOZTAv2"
      },
      "source": [
        "## Why are we already talking about model evaluation?\n",
        "\n",
        " *   We need a way to choose between different model types, tuning parameters, and features\n",
        " *   Model evaluation enables us to estimate how well a model will generalize to out-of-sample data\n",
        " *   A model evaluation metric (or many!) required to quantify the model performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fwJVWtTVTWbz"
      },
      "source": [
        "## Model training procedure:\n",
        "1. Split the dataset into a **training** set and **testing** set.\n",
        "> Put the testing data set aside and **do not use it for any part of model training**.\n",
        "2. Further split your **training** set into a **training** and **validation** set.\n",
        "3. Train the model on only the **training** data set.\n",
        "4. Evaluate model performance on the **validation** set.\n",
        "5. Repeat steps 2+ as necessary.\n",
        "6. Only once the model has been finalized, should you evaluate on the left-out **testing** set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QoU7RS38TWmr"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI37q_ITj7y-",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we will work with the [PIMA diabetes dataset](https://www.kaggle.com/uciml/pima-indians-diabetes-database/version/1). The goal is to predict whether or not a given female patient will contract diabetes based on certain diagnostic measurements that were recorded.\n",
        ">This is a ***binary*** classification problem. Remember, an outcome value of 0 indicates that the patient does not have diabetes, while an outcome of 1 indicates that the patient does have diabetes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDBcOlS5kFN0",
        "colab_type": "text"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ym-sI1JMfPT3",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bX7xCiaESvv2",
        "colab": {}
      },
      "source": [
        "# Loading dataset\n",
        "url = \"https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/NotebookExampleData/Week1/diabetes.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "\n",
        "data = pd.read_csv(url, names=names)\n",
        "\n",
        "# Dropping NaN rows\n",
        "invalid = ['plas', 'pres', 'skin', 'test', 'mass']\n",
        "\n",
        "# Replace 0's with NaN's in the \"invalid\" columns\n",
        "for i in invalid:\n",
        "    data[i].replace(to_replace=0, value=np.nan, inplace=True)\n",
        "\n",
        "# Drop rows with NaN's, and drop the resultant indexing column\n",
        "data = data.dropna(axis=0).reset_index(drop=True)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UeWjHRSd6HhQ"
      },
      "source": [
        "### Split data into training, validation, and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zJPvQo9kdgKb",
        "colab": {}
      },
      "source": [
        "# Define your features (X) and labels (y)\n",
        "X = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']\n",
        "y = 'class'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2jJdDuQystTL",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split X and y into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[X], data[y], test_size=0.2, random_state=0)\n",
        "\n",
        "# Further split X and y of training into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kYzKhOU36Ms9"
      },
      "source": [
        "### Train a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doZ1v_0rkz1e",
        "colab_type": "text"
      },
      "source": [
        "Here we make use of a Decision Tree model. You don't have to worry about understanding the details for now; we'll be covering decision trees and other models more in depth later this week."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8-x6Uhcas3kn",
        "colab": {}
      },
      "source": [
        "# Train a decision tree model on the training set only\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = DecisionTreeClassifier(max_depth=4,random_state=0)\n",
        "model.fit(X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CumutApe6SdZ"
      },
      "source": [
        "## Model Evalutaion on Validation Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HftPrleIlR3p",
        "colab_type": "text"
      },
      "source": [
        "For now, we will use the model to predict the binary classes of our validation data. Later we'll look at the case where our model outputs a probability over class labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GtJigb1SuTRF",
        "colab": {}
      },
      "source": [
        "# Make class predictions for the training validation set\n",
        "y_train_predict = model.predict(X_train)\n",
        "y_val_predict = model.predict(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVGSjhvXlgBw",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WuOp6VQ4wq9C",
        "colab": {}
      },
      "source": [
        "# Calculate model training accuracy\n",
        "from sklearn import metrics\n",
        "print('Training Accuracy:   {:01.3f}'.format(metrics.accuracy_score(y_train, y_train_predict)))\n",
        "\n",
        "# Calculate model validation accuracy\n",
        "print('Validation Accuracy: {:01.3f}'.format(metrics.accuracy_score(y_val, y_val_predict)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_NEuINphjha5"
      },
      "source": [
        "Notice that our **training accuracy** is higher than our **validation accuracy**.  This is fairly typical, as  machine learning algorithms have a tendency to overfit the training data.  When choosing a model, it's a good idea to tune it such that you are not overfitting too much.\n",
        "\n",
        "![](https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/Images/Week1/fittings.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xLEkVGUX1XJY"
      },
      "source": [
        "Okay, so our decision tree got ~78% validation accuracy.  But how do we know if that's a \"good\" accuracy?  Let's look at the simple case where we have a model that always predicts the most common class.  How well would that do?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I3dLk7Gozt6j",
        "colab": {}
      },
      "source": [
        "# Examine the class distribution of the validation set (using a Pandas Series method)\n",
        "y_val.value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIVg21JAmCr_",
        "colab_type": "text"
      },
      "source": [
        "This means that a trivial model that always predicts 0 would be right ~63% of the time. So our decision tree seems to be doing something. \n",
        "\n",
        "However what if your training and validation dataset happened to have a bigger class imbalance? Say an imbalance of 90% negative and only 10% positive? If your model constantly predicts 0 you'd technically have a *really* good accuracy of 90%, but basing your metric on this metric alone can be misleading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6tVEgqHmEk-",
        "colab_type": "text"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ollw--0a0FuN"
      },
      "source": [
        "\n",
        "\n",
        "Now let's investigate the errors our model is making by looking at the **confusion matrix.** This is a simple way to tabulate the kinds of results our model is producing. It will follow the format shown below:\n",
        "\n",
        "![](https://raw.githubusercontent.com/BeaverWorksMedlytics2020/Data_Public/master/Images/Week1/confusion%20matrix%202.png)\n",
        "\n",
        "In this binary example shown above, are two possible predicted classes: negative (0) and positive (1). If we were predicting the presence of a disease, for example, positive would mean they have the disease, and negative would mean they don't have the disease.\n",
        "\n",
        "- true positives (TP): Your model predicted positive, and the true label *is* positive\n",
        "- true negatives (TN): Your model predicted negative, and the true label *is* negative\n",
        "- false positives (FP): Your model predicted positive, but the true label is actually negative (Also known as a \"Type I error\")\n",
        "- false negatives (FN): Your model predicted negative, but true label is actually positive (Also known as a \"Type II error\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qw3xcAK3z5t-",
        "colab": {}
      },
      "source": [
        "# Use sklearn to make the confusion matrix\n",
        "print('Confusion Matrix:')\n",
        "labels = [0,1]\n",
        "cm = metrics.confusion_matrix(y_val, y_val_predict, labels)\n",
        "print(cm)\n",
        "\n",
        "# Normalized confusion matrix\n",
        "print('\\nNormalized Confusion Matrix:')\n",
        "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "print(cm_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eVNTxWYB1obX",
        "colab": {}
      },
      "source": [
        "# Or you can use pandasâ€”it's a little nicer to look at\n",
        "pd.crosstab(y_val, y_val_predict, rownames=['True'], colnames=['Predicted'], margins=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaKtoM3H8fhx",
        "colab_type": "text"
      },
      "source": [
        "We can also use a color heatmap to compare the frequency of each result type at a glance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UXUOndg_1lGf",
        "colab": {}
      },
      "source": [
        "# Visualize the confusion matrix\n",
        "def plot_cmatrix(cm,labels,title='Confusion Matrix'):\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111)\n",
        "  cax = ax.matshow(cm,cmap='Reds')\n",
        "  plt.title('\\n'+title+'\\n', fontsize=20)\n",
        "  fig.colorbar(cax)\n",
        "  ax.set_xticklabels([''] + labels, fontsize=16)\n",
        "  ax.set_yticklabels([''] + labels, fontsize=16)\n",
        "  plt.xlabel('Predicted', fontsize=16)\n",
        "  plt.ylabel('True', fontsize=16)\n",
        "  plt.show()\n",
        "  \n",
        "plot_cmatrix(cm,labels)\n",
        "plot_cmatrix(cm_norm,labels,title='Normalized Confusion Matrix')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hczAcaUe4YP6"
      },
      "source": [
        "You can even make a confusion matrix for a multi-class problem! All we need to do is increase the number of squares, since there are more ways that a prediction can be right or wrong. Consider this example, predicting types of animals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w2f3hsH04YiW",
        "colab": {}
      },
      "source": [
        "# This series represents the true data\n",
        "y2=pd.Series(['dog', 'cat', 'dog', 'dog', 'dog', 'cat', 'cat', 'cat', 'dog', 'cat', 'dog', 'frog', 'dog', 'dog', 'cat', 'frog'])\n",
        "# This series represents a set of predictions\n",
        "y2pred=pd.Series(['dog', 'cat', 'dog', 'dog', 'cat', 'frog', 'cat', 'cat', 'dog', 'cat', 'dog', 'frog', 'frog', 'dog', 'cat', 'frog'])\n",
        "\n",
        "animal_labels=['cat','dog','frog']\n",
        "cm = metrics.confusion_matrix(y2,y2pred, animal_labels)\n",
        "plot_cmatrix(cm,animal_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1Fkg_EdKahkl"
      },
      "source": [
        "There are several useful metrics that are derived from the confusion matrix:\n",
        "* sensitivity, recall, hit rate, or **true positive rate (TPR)** : $ \\mathrm {TPR} ={\\frac {\\mathrm {TP} }{P}}={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FN} }}$\n",
        " \n",
        "* precision or **positive predictive value (PPV)** : $ \\mathrm {PPV} ={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FP} }}$\n",
        "\n",
        "* specificity or **true negative rate (TNR)** : $\\mathrm {TNR} ={\\frac {\\mathrm {TN} }{N}}={\\frac {\\mathrm {TN} }{\\mathrm {TN} +\\mathrm {FP} }}$\n",
        "\n",
        "* miss rate or **false negative rate (FNR)** : $ \\mathrm {FNR} ={\\frac {\\mathrm {FN} }{P}}={\\frac {\\mathrm {FN} }{\\mathrm {FN} +\\mathrm {TP} }}=1-\\mathrm {TPR}$\n",
        "\n",
        "* fall-out or **false positive rate (FPR)** : $\\mathrm {FPR} ={\\frac {\\mathrm {FP} }{N}}={\\frac {\\mathrm {FP} }{\\mathrm {FP} +\\mathrm {TN} }}=1-\\mathrm {TNR} $\n",
        "\n",
        "* **accuracy (ACC)** : $\\mathrm {ACC} ={\\frac {\\mathrm {TP} +\\mathrm {TN} }{P+N}}={\\frac {\\mathrm {TP} +\\mathrm {TN} }{\\mathrm {TP} +\\mathrm {TN} +\\mathrm {FP} +\\mathrm {FN} }}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6lQJ3pmtlIP",
        "colab_type": "text"
      },
      "source": [
        "## F1 score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQJkKrtktJV3",
        "colab_type": "text"
      },
      "source": [
        "The **F1 score** is the harmonic mean of precision and recall. The best possible F1 score would be 1, and worst would be 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
        " * F1 score: $F_{1}=2\\cdot {\\frac {\\mathrm {PPV} \\cdot \\mathrm {TPR} }{\\mathrm {PPV} +\\mathrm {TPR} }}={\\frac {2\\mathrm {TP} }{2\\mathrm {TP} +\\mathrm {FP} +\\mathrm {FN} }}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgZG70Ak9onY",
        "colab_type": "text"
      },
      "source": [
        "Let's calculate the precision, recall, and F1 values for the model we created earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iLRgnLA11eT-",
        "colab": {}
      },
      "source": [
        "print('Precision: {:01.3f}'.format(metrics.precision_score(y_val,y_val_predict)))\n",
        "print('Recall:    {:01.3f}'.format(metrics.accuracy_score(y_val,y_val_predict)))\n",
        "print('F1 score:  {:01.3f}'.format(metrics.f1_score(y_val,y_val_predict)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fY2Bf6446CT8"
      },
      "source": [
        "## Outputting Probabilities "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPu3EF5lXEOW",
        "colab_type": "text"
      },
      "source": [
        "Up to now, we've simply been evaluating our model's ability to predict the correct class. But in most practical cases, we will want a model to output a *probability* or *certainty* rather than class labels. Fortunately, the sklearn model we created can already calculate probabilities for us. Let's consider the classification probabilities for a single entry."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nzlcegvR1a35",
        "colab": {}
      },
      "source": [
        "# Predict class label probabilities\n",
        "labels = [0,1]\n",
        "y_val_prob = model.predict_proba(X_val,labels)\n",
        "\n",
        "# Output predicted and true values for the first validation point\n",
        "print('Probabilities:\\n',y_val_prob[0])\n",
        "print('\\nTrue Value:\\n',y_val.values[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-0_TqOn0Bh53"
      },
      "source": [
        "Notice that the decision tree predicts that there is a ~78% probability that this entry belongs to class label 1. This doesn't necessarily mean we *have* to label the entry as class 1. For example, we may decide to say that 90% confidence is required for an entry to be assigned class 1, otherwise it belongs to class 0. The predicted class will depend on where we threshold our decision boundary.\n",
        "\n",
        "Choosing a threshold is not always as straighforward as choosing the label with the greatest probability. If we know a data set to be biased, or if there is a greater penalty for either false positives or false negatives, it can often be desirable to evaluate the model over a range of thresholds. This is where the **Receiver Operating Characteristic (ROC) curve** comes in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIayB7VAkI9T",
        "colab_type": "text"
      },
      "source": [
        "### ROC Curves and AUC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peOVwo3QkHyO",
        "colab_type": "text"
      },
      "source": [
        "The **Receiver Operating Characteristic curve**, or **ROC curve**, is a plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. That is to say, it shows how good the classifier is at different thresholds. It is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxWpfsvFkw7A",
        "colab_type": "text"
      },
      "source": [
        "Often, the **area under the (ROC) curve,** or **AUC**, is used as a robust and descriptive metric of a classification model's performance. \n",
        ">An AUC of 1 is \"perfect,\" and an AUC of .5 means that the classifier is no better than random.\n",
        "\n",
        "To describe what the AUC means, consider that a model's prediction of the chance for an entry to be in a class to be that entry's \"score.\" Then, the AUC is the probability that a randomly chosen entry that truly is in that class will have a higher \"score\" than a random entry that is not in that class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1oLYVkqEyWIr",
        "colab": {}
      },
      "source": [
        "# Calculate the FPR and TPR at varying thresholds (assume label 1 is the \"postive\" class)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_val, y_val_prob[:,1])\n",
        "\n",
        "# Calculate the area under the ROC curve\n",
        "roc_auc = metrics.auc(fpr, tpr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "anbcgur8ypbj",
        "colab": {}
      },
      "source": [
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(6,6))\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjn5oEwODYoU",
        "colab_type": "text"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "There is no separate exercise notebook for this section, but here are a few things to do on your own:\n",
        "* Perform the same analysis above using [k-fold cross validation](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)\n",
        "* Choose a \"good\" threshold for the decision tree created above (according to some metric, e.g., minimizing False Positives)\n",
        "* Evaluate the TEST data using the model at your chosen threshold -- how does the classifier perform?\n",
        "\n",
        "*Note: This tutorial is derived from Data School's Machine Learning with scikit-learn tutorial.*"
      ]
    }
  ]
}
